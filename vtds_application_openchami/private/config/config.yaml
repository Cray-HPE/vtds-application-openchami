#
# MIT License
#
# (C) Copyright 2025 Hewlett Packard Enterprise Development LP
#
# Permission is hereby granted, free of charge, to any person obtaining a
# copy of this software and associated documentation files (the "Software"),
# to deal in the Software without restriction, including without limitation
# the rights to use, copy, modify, merge, publish, distribute, sublicense,
# and/or sell copies of the Software, and to permit persons to whom the
# Software is furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included
# in all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL
# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR
# OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
# ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
# OTHER DEALINGS IN THE SOFTWARE.
#
application:
  # The deployment section specifies which of the available deployment
  # modes is to be used to deploy OpenCHAMI and any configuration
  # specific to that deployment mode that might be needed.
  #
  # The modes are:
  #
  #   'quadlet'    - a deployment mode that deploys OpenCHAMI into
  #                  systemd managed podman quadlets loosely based on
  #                  the OpenCHAMI 2025 Tutorial, but with support for
  #                  a cluster of Virtual Compute Nodes running directly
  #                  on Virtual Blade hosts and with support for
  #                  Magellan discovery of those Virtual Compute Nodes.
  #
  #   'bare'       - a deployment that finishes without OpenCHAMI
  #                  deployed at all, but with a management node
  #                  deployed that is ready for the 2025 Tutorial to be
  #                  run on it.
  deployment:
    mode: quadlet

  # The host block identifies the Virtual Network on which OpenCHAMI is
  # hosted and the Virtual Node class whose instances will run
  # OpenCHAMI. Deployment will be done to all nodes in that class (for a
  # single instance of OpenCHAMI the cluster configuration should
  # specify only one instance of the host node class).
  host:
    # The Virtual Network (named in the Cluster layer configuration) on
    # which OpenCHAMI will be hosted.
    network: hostnet
    # The node class (named in the Cluster layer configuration) on which
    # OpenCHAMI will be deployed and run.
    node_class: host_node
    # The host nodename will identify the host node within the cluster's
    # DNS domain. For example, if your cluster has the domain name
    # 'openchami.cluster' and the host node name specified here is
    # 'demo' then the head node of the cluster will be set up as
    #
    #     demo.openchami.cluster.
    #
    # Those defaults are used here.
    node_name: demo
  # The cluster block contains configuration pertaining to the cluster
  # as whole.
  cluster:
    # The cluster domain name is used for resolving the DNS names of
    # nodes within the OpenCHAMI cluster.
    domain_name: openchami.cluster
  # There are two DNS servers used beyond the cluster DNS server that is
  # automatically set up using smd-coredns. The first contains the
  # resolution of the head node FQDN (example: demo.openchami.cluster)
  # for use within the cluster and, ostensibly, outside the cluster. If
  # this is not specified or NULL it is  filled in with the blade-local
  # network IP address of the host Virtual Blade where the management
  # node is hosted. If this is specified it should point to a DNS server
  # somewhere that knows the head-node FQDN. The second is the public
  # DNS server. Generally speaking it is good to use whatever DNS server
  # might be provided by the platform layer, if such a thing
  # exists. Otherwise configure any public DNS server IP here.
  dns:
    # For the site DNS server we generally use the host blade network IP
    # address of the Virtual Blade hosting the management node. This is
    # a likely value for that. If yours differs, override this value in
    # your configuration.
    site: 10.234.0.1
    # this is IP of the GCP metadata server override which works for GCP
    # as a provider. If another provider is used, override this in your
    # configuration overlay.
    #
    # XXX - it might be beter to move the setting for this down into the
    #       provider layer and pull it from there using a new provider
    #       layer API, but this will serve for now.
    public: 169.254.169.254
  # The discovery_networks map is a collection of network specifications
  # for networks on which OpenCHAMI (currently Magellan) may do
  # discovery of RedFish endpoints. A discovery network may be either a
  # Cluster Network, identified by a network name from the Cluster layer
  # configuration, or an internal network, with no network name but an
  # explicit network CIDR. Either a network CIDR or a network name must
  # be supplied. If both or neither is supplied this is an error. If a
  # given cluster does not contain the network named in a Cluster
  # Network, that network will be removed from the set of discovery
  # networks prior to deployment.
  #
  # All of the valid networks specified here will be used for discovery
  # of RedFish endpoints when OpenCHAMI is deployed.
  discovery_networks:
    hardware_management:
      network_cidr: null
      network_name: hardware_management
      # Username and password used by RedFish on the Virtual Blades. The
      # password here grants access to the Virtual Nodes of the cluster,
      # so it is a bit more sensitive. It is generated and stuffed into
      # the configuration automatically if it is null. Override it at your
      # own risk if you want a known value.
      redfish_username: root
      redfish_password: root_password
      # The list of Virtual Blade classes here tells the management node
      # deployment which blade classes to include in the set of RedFish
      # endpoints that need mappings to xnames.
      blade_classes:
        - host-blade
